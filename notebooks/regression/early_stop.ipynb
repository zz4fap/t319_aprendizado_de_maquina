{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "!git clone https://github.com/zz4fap/t319_aprendizado_de_maquina.git\n",
    "import sys\n",
    "sys.path.insert(0,'./t319_aprendizado_de_maquina/projeto/')\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Reset PN sequence generator.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Number of examples.\n",
    "N = 100\n",
    "\n",
    "# Generate dataset.\n",
    "x_train, y_train, y_train_noisy, x_test, y_test_noisy = util.generateDatasets3(N)\n",
    "\n",
    "# Plot comparison between true and noisy model.\n",
    "plt.plot(x_train, y_train_noisy, '.', label='Noisy signal')\n",
    "plt.plot(x_train, y_train, label='True signal', linewidth=4)\n",
    "plt.xlabel('$x$', fontsize=14)\n",
    "plt.ylabel('$y$', fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateDatasets3(N):\n",
    "    # Attribute.\n",
    "    x_train = np.sort(3*np.random.rand(N, 1) - 1.5, axis=0)\n",
    "    # True function.\n",
    "    y_train = 1 + x_train + 0.5*x_train**2 - 1.5*x_train**3\n",
    "    # Observable function.\n",
    "    y_train_noisy = y_train + np.random.randn(N, 1)\n",
    "    \n",
    "    # Attribute.\n",
    "    N = 50\n",
    "    x_test = np.linspace(-1.5,1.5,N).reshape(N,1)\n",
    "    # True function.\n",
    "    y_test = 1 + x_test + 0.5*x_test**2 - 1.5*x_test**3\n",
    "    # Observable function.\n",
    "    y_test_noisy = y_test + np.random.randn(N, 1)    \n",
    "    \n",
    "    return x_train, y_train, y_train_noisy, x_test, y_test_noisy\n",
    "\n",
    "def gradientDescentMiniBatch(X_train, y_train_noisy, X_test, y_test_noisy, alpha, n_epochs, N, mb_size, seed):\n",
    "    '''\n",
    "    Função que implementa o algoritmo do gradiente descendente em mini-batches.\n",
    "    Parâmetros de entrada:\n",
    "       X_train: matriz de atributos de treinamento\n",
    "       y_train_noisy: vetor de rótulos de treinamento (ou valores esperados)\n",
    "       X_test: matriz de atributos de validação\n",
    "       y_test_noisy: vetor de rótulos de validação (ou valores esperados) \n",
    "       alpha: passo de aprendizagem\n",
    "       n_epochs: número máximo de épocas\n",
    "       N: número de exemplos do conjunto de treinamento\n",
    "       mb_size: tamanho do mini-batch\n",
    "    '''\n",
    "    # Reset PN sequence generators.\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Define the initial minimum error.\n",
    "    error_min = float(\"inf\")\n",
    "    \n",
    "    # Random initialization of parameters.\n",
    "    a = np.random.randn(X_train.shape[1], 1)\n",
    "\n",
    "    # Create vector for parameter history.\n",
    "    a_hist = np.zeros((X_train.shape[1], n_epochs*(N//mb_size)+1))\n",
    "    # Initialize history vector.\n",
    "    a_hist[:, 0] = a.reshape(X_train.shape[1],)\n",
    "\n",
    "    # Create array for storing error values.\n",
    "    Jgd_train = np.zeros(n_epochs*(N//mb_size)+1)\n",
    "    Jgd_test = np.zeros(n_epochs*(N//mb_size)+1)\n",
    "\n",
    "    Jgd_train[0] = (1.0/N)*sum(np.power(y_train_noisy - X_train.dot(a), 2))\n",
    "    Jgd_test[0] = (1.0/N)*sum(np.power(y_test_noisy - X_test.dot(a), 2))\n",
    "\n",
    "    # Create array for storing gradient values.\n",
    "    grad_hist = np.zeros((X_train.shape[1], n_epochs*(N//mb_size)))\n",
    "\n",
    "    # Mini-batch gradient-descent loop.\n",
    "    inc = 0\n",
    "    for e in range(n_epochs):\n",
    "\n",
    "        # Shuffle the whole dataset before every epoch.\n",
    "        shuffled_data_set_indexes = random.sample(range(0, N), N)\n",
    "\n",
    "        for i in range(0, N//mb_size):\n",
    "\n",
    "            start = i*mb_size\n",
    "            end = mb_size*(i+1)\n",
    "            batch_indexes = shuffled_data_set_indexes[start:end]\n",
    "\n",
    "            xi = X_train[batch_indexes]\n",
    "            yi = y_train_noisy[batch_indexes]\n",
    "\n",
    "            gradients = -(2.0/mb_size)*xi.T.dot(yi - xi.dot(a))\n",
    "            a = a - alpha*gradients\n",
    "\n",
    "            Jgd_train[inc+1] = (1.0/N)*sum(np.power((y_train_noisy - X_train.dot(a)), 2))\n",
    "            Jgd_test[inc+1] = (1.0/N)*sum(np.power((y_test_noisy - X_test.dot(a)), 2))\n",
    "            \n",
    "            if(Jgd_test[inc+1] < error_min):\n",
    "                error_min = Jgd_test[inc+1]\n",
    "                a_min = a\n",
    "                inc_min = inc\n",
    "            \n",
    "            grad_hist[:, inc] = gradients.reshape(X_train.shape[1],)\n",
    "            a_hist[:, inc+1] = a.reshape(X_train.shape[1],)\n",
    "\n",
    "            inc = inc + 1\n",
    "            \n",
    "    return a, Jgd_train, Jgd_test, a_hist, grad_hist, inc, error_min, a_min, inc_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ao executar a função `gradientDescentMiniBatch` você encontrar o warning: `RuntimeWarning: overflow encountered in power`, diminua o valor do passo de aprendizagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 30\n",
    "include_bias = True\n",
    "n_epochs = 100000\n",
    "mb_size = N\n",
    "alpha = 0.003\n",
    "\n",
    "# Transform attribute vector into polynomial attribute matrix.\n",
    "poly = PolynomialFeatures(degree=degree, include_bias=include_bias)\n",
    "X_train_poly = poly.fit_transform(x_train)\n",
    "X_test_poly = poly.fit_transform(x_test)\n",
    "\n",
    "# Instantiate a scaler that will standardize the features.\n",
    "scaler = StandardScaler()\n",
    "X_train = np.c_[np.ones((N,1)), scaler.fit_transform(X_train_poly[:,1:])]\n",
    "X_test = np.c_[np.ones((int(N/2),1)), scaler.transform(X_test_poly[:,1:])]\n",
    "\n",
    "a, Jgd_train, Jgd_test, a_hist, grad_hist, inc, error_min, a_min, inc_min = gradientDescentMiniBatch(X_train, y_train_noisy, X_test, y_test_noisy, alpha, n_epochs, N, mb_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argwhere(Jgd_test == min(Jgd_test))[0][0])\n",
    "print(inc_min)\n",
    "print(error_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot contour figure.        \n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(0, inc+1), Jgd_train[0:inc+1], label='Treinamento')\n",
    "plt.plot(np.arange(0, inc+1), Jgd_test[0:inc+1], label='Validação')\n",
    "plt.xlim((0, inc+1))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteração', fontsize=14)\n",
    "plt.ylabel('$J_e$', fontsize=14)\n",
    "plt.title('Erro vs. Iteração')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "if(0):\n",
    "    maxX = 2000\n",
    "    left, bottom, width, height = [0.2, 0.5, 0.3, 0.3]\n",
    "    ax1 = fig.add_axes([left, bottom, width, height])\n",
    "    ax1.plot(np.arange(0, maxX), Jgd_train[0:maxX])\n",
    "    ax1.plot(np.arange(0, maxX), Jgd_test[0:maxX])\n",
    "    ax1.grid()\n",
    "    ax1.set_yscale('log')\n",
    "    #ax1.set_ylim(0, maxY)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_min = X_train.dot(a_min)\n",
    "y_test_pred_min = X_test.dot(a_min)\n",
    "\n",
    "# Plot comparison between true and noisy model.\n",
    "plt.plot(x_train, y_train_noisy, '.', label='Noisy signal')\n",
    "plt.plot(x_train, y_train, label='True signal', linewidth=4)\n",
    "plt.plot(x_train, y_train_pred_min, label='Treinamento')\n",
    "plt.plot(x_test, y_test_pred_min, label='Validação')\n",
    "plt.ylim(min(y_train_noisy)-0.05, max(y_train_noisy)+0.05)\n",
    "plt.xlabel('$x$', fontsize=14)\n",
    "plt.ylabel('$y$', fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
