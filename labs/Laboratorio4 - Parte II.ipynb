{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório #4 - Parte II\n",
    "\n",
    "### Instruções\n",
    "\n",
    "1. Quando você terminar os exercícios do laboratório, vá ao menu do Jupyter ou Colab e selecione a opção para fazer download do notebook.\n",
    "    * Os notebooks tem extensão .ipynb. \n",
    "    * Este deve ser o arquivo que você irá entregar.\n",
    "    * No Jupyter vá até a opção **File** -> **Download as** -> **Notebook (.ipynb)**.\n",
    "    * No Colab vá até a opção **File** -> **Download .ipynb**.\n",
    "2. Após o download do notebook, vá até a aba de tarefas do MS Teams, localize a tarefa referente a este laboratório e faça o upload do seu notebook. Veja que há uma opção de anexar arquivos à tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nome**:\n",
    "\n",
    "**Matrícula**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Neste exercício, você irá encontrar uma solução para o problema de regressão linear com a versão estocástica do gradiente descendente.\n",
    "\n",
    "O código da célula abaixo, além de conter a definição de algumas funções que iremos utilizar neste exercício, gera valores da seguinte **função observável**\n",
    "\n",
    "$$y_{noisy}(n) = y(n) + w(n),$$\n",
    "\n",
    "onde $w$ é vetor coluna com $N = 1000$ (ou seja, o número de exemplos) valores retirados de uma distribuição aleatória Gaussiana Normal Padrão (i.e., com média zero e variância unitária) e $y$ é a **função objetivo**. Neste exercício, a **função objetivo** (ou **modelo gerador**) é dada por:\n",
    "\n",
    "$$y(n) = x_1(n) + x_2(n),$$\n",
    "\n",
    "onde $x_1$ e $x_2$ são vetores coluna com $N$ valores retirados da distribuição Gaussiana Normal Padrão.\n",
    "\n",
    "A **função hipótese** para este exercício é dada por\n",
    "\n",
    "$$h(n) = \\hat{a}_1 x_1(n) + \\hat{a}_2 x_2(n).$$\n",
    "\n",
    "De posse destas informações, faça o seguinte:\n",
    "\n",
    "#### A. Execute a célula de código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reset the pseudo random number generator to the same value.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Generate points for plotting the error surface.\n",
    "def calculateErrorSurface(X, y_noisy):\n",
    "    N = len(y_noisy)\n",
    "    # Generate values for parameters.\n",
    "    M = 200\n",
    "    a1 = np.linspace(-20.0, 24.0, M)\n",
    "    a2 = np.linspace(-20.0, 24.0, M)\n",
    "\n",
    "    A1, A2 = np.meshgrid(a1, a2)\n",
    "\n",
    "    # Generate points for plotting the cost-function surface.\n",
    "    J = np.zeros((M,M))\n",
    "    for iter1 in range(0, M):\n",
    "        for iter2 in range(0, M):\n",
    "            yhat = A1[iter1][iter2]*x1 + A2[iter1][iter2]*x2\n",
    "            J[iter1][iter2] = (1.0/N)*np.sum(np.square(y_noisy - yhat))\n",
    "    return J, A1, A2\n",
    "\n",
    "# Closed-form solution.\n",
    "def normalEquationSolution(X, y_noisy):\n",
    "    N = len(y_noisy)\n",
    "    a_opt = np.linalg.pinv(np.transpose(X).dot(X)).dot(np.transpose(X).dot(y_noisy))\n",
    "    yhat = X.dot(a_opt)\n",
    "    Joptimum = (1.0/N)*np.sum(np.power((y_noisy - yhat), 2))\n",
    "    return a_opt, Joptimum\n",
    "\n",
    "# Plot performance results.\n",
    "def plotResults(Jgd, J, A1, A2, a_opt, iteration, maxX, maxY):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "    cp = ax1.contour(A1, A2, J)\n",
    "    ax1.clabel(cp, inline=1, fontsize=10)\n",
    "    ax1.set_xlabel('$a_1$', fontsize=14)\n",
    "    ax1.set_ylabel('$a_2$', fontsize=14)\n",
    "    ax1.set_title('Cost-function\\'s Contour')\n",
    "    ax1.plot(a_opt[0], a_opt[1], c='r', marker='*', markersize=14)\n",
    "    ax1.plot(a_hist[0, 0:iteration], a_hist[1, 0:iteration], 'k--')\n",
    "    ax1.plot(a_hist[0, 0:iteration], a_hist[1, 0:iteration], 'kx')\n",
    "    ax1.set_xticks(np.arange(-20, 24, step=4.0))\n",
    "    ax1.set_yticks(np.arange(-20, 24, step=4.0))\n",
    "\n",
    "    ax2.plot(np.arange(0, iteration), Jgd[0:iteration])\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Iteration', fontsize=14)\n",
    "    ax2.set_ylabel('$J_e$', fontsize=14)\n",
    "    ax2.set_title('Error vs. Epoch number')\n",
    "    ax2.set_xlim((0, iteration-1))\n",
    "\n",
    "    left, bottom, width, height = [0.65, 0.5, 0.23, 0.3]\n",
    "    ax3 = fig.add_axes([left, bottom, width, height])\n",
    "    ax3.plot(np.arange(0, maxX), Jgd[0:maxX])\n",
    "    ax3.set_ylim(0, maxY)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Gradient descent solution.\n",
    "def gradientDescent(X, y_noisy, n_epochs, mb_size, seed):\n",
    "    # Number of examples.\n",
    "    N = len(y_noisy)\n",
    "    \n",
    "    # Reset PN generator.\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Random initialization of parameters.\n",
    "    a = np.array([-20.0, -20.0]).reshape(2, 1)\n",
    "\n",
    "    # Create vector for parameter history.\n",
    "    a_hist = np.zeros((2, n_epochs*(N//mb_size)+1))\n",
    "    # Initialize history vector.\n",
    "    a_hist[:, 0] = a.reshape(2,)\n",
    "\n",
    "    # Create array for storing error values.\n",
    "    Jgd = np.zeros(n_epochs*(N//mb_size)+1)\n",
    "\n",
    "    Jgd[0] = (1.0/N)*sum(np.power(y_noisy - X.dot(a), 2))\n",
    "\n",
    "    # Create array for storing gradient values.\n",
    "    grad_hist = np.zeros((2, n_epochs*(N//mb_size)))\n",
    "\n",
    "    # Gradient-descent loop.\n",
    "    inc = 0\n",
    "    for e in range(n_epochs):\n",
    "\n",
    "        # Shuffle the whole dataset before every epoch.\n",
    "        shuffled_indexes = random.sample(range(0, N), N)\n",
    "        \n",
    "        for i in range(0, N//mb_size):\n",
    "\n",
    "            start = i*mb_size\n",
    "            end = mb_size*(i+1)\n",
    "            indexes = shuffled_indexes[start:end]\n",
    "\n",
    "            xi = X[indexes]\n",
    "            yi = y_noisy[indexes]\n",
    "\n",
    "            gradients = -(2.0/mb_size)*xi.T.dot(yi - xi.dot(a))\n",
    "            a = a - alpha*gradients\n",
    "\n",
    "            Jgd[inc+1] = (1.0/N)*sum(np.power((y_noisy - X.dot(a)), 2))\n",
    "\n",
    "            grad_hist[:, inc] = gradients.reshape(2,)\n",
    "            a_hist[:, inc+1] = a.reshape(2,)\n",
    "\n",
    "            inc += 1\n",
    "            \n",
    "    return a, Jgd, a_hist, grad_hist, inc\n",
    "\n",
    "# Number of examples\n",
    "N = 1000\n",
    "\n",
    "# Input values (features)\n",
    "x1 = np.random.randn(N, 1)\n",
    "x2 = np.random.randn(N, 1)\n",
    "\n",
    "# Noise.\n",
    "w = np.random.randn(N, 1)\n",
    "\n",
    "# True model.\n",
    "y = x1 + x2\n",
    "\n",
    "# Observable function.\n",
    "y_noisy = y + w\n",
    "\n",
    "# Concatenate both feature vectors.\n",
    "X = np.c_[x1,x2]\n",
    "\n",
    "# Generate values for plotting the error surface.\n",
    "J, A1, A2 = calculateErrorSurface(X, y_noisy)\n",
    "\n",
    "# Calculate optimum parameters with closed-form.\n",
    "a_opt, Joptimum = normalEquationSolution(X, y_noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Na sequência, faça o seguinte:\n",
    "\n",
    "   1. Copie o código do item 5 do exercício 1 da parte I do laboratório 4 na célula abaixo.\n",
    "   2. Altere o parâmetro da função `gradientDescent` que configura o tamanho do mini-batch para que você use a versão **estocástica** do gradiente descendente.\n",
    "   3. Altere o tamanho do mini-batch (`mb_size`) para $1$.\n",
    "   4. Altere o passo de aprendizagem para $0.1$.\n",
    "   5. Altere os 2 últimos parâmetros de entrada da função `plotResults` para $200$ e $10$, respectivamente. Veja abaixo como a função deve ficar:\n",
    "```python\n",
    "plotResults(Jgd, J, A1, A2, a_opt, iteration, 200, 10)\n",
    "```\n",
    "   6. Em seguida, execute a célula e analise o resultado obtido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cole o código aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. O que você pode dizer sobre o comportamento do algoritmo em relação à versão em batelada do gradiente descendente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Escreva aqui sua análise sobre o comportamento do gradiente descendente estocástico quando comparado com a versão em batelada.\n",
    "\n",
    "* Resposta:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Neste exercício, você irá usar a classe `SGDRegressor` da biblioteca SciKit-Learn para resolver um problema de regressão linear. \n",
    "\n",
    "O link abaixo contém a versão mais recente da documentação da classe `SGDRegressor`. Antes de resolver o exercício, faça uma leitura rápida da documentação. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "\n",
    "**DICA**: \n",
    "\n",
    "* Use o exemplo `SGD_with_scikit_learn_lib.ipynb` como modelo para resolução deste exercício.\n",
    "\n",
    "Em seguida, faça o seguinte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Analise e execute o trecho de código abaixo para gerar o conjunto de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de exemplos.\n",
    "N = 1000\n",
    "\n",
    "# Atributo.\n",
    "x1 = np.random.randn(N, 1)\n",
    "\n",
    "# Ruído.\n",
    "w = np.random.randn(N, 1)\n",
    "\n",
    "# Modelo gerador.\n",
    "y = 2 + 4*x1\n",
    "\n",
    "# Função observável.\n",
    "y_noisy = y + w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Crie um gráfico comparando os valores originais e ruidosos do modelo gerador. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve o código aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Na célula abaixo, importe a classe `SGDRegressor` e instancia um objeto desta classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve o código aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Treine o modelo de regressão linear utilizando o método `fit` do objeto da classe `SGDRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve o código aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. De posse do modelo treinado, use o método `predict` para prever os valores de saída do modelo usando como entrada o atributo `x1`. Armazene o resultado da previsão em uma variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve o código aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. Usando os valores esperados e os valores de saída do modelo de regressão linear, calcule e imprima o erro quadrático médio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve o código aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### G. Imprima os valores dos pesos do modelo obtidos com o treinamento através do gradiente descendente estocástico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve o código aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H. Crie um gráfico comparando os valores originais, ruidosos e gerados pelo modelo de regressão linear obtido com o gradiente descendente estocástico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve o código aqui."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
